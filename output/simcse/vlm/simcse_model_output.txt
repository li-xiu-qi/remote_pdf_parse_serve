<|box_start|>168 090 830 111<|box_end|><|ref_start|>title<|ref_end|><|md_start|># SimCSE: Simple Contrastive Learning of Sentence Embeddings<|md_end|>
<|box_start|>206 131 799 215<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Tianyu Gao†* Xingcheng Yao†* Danqi Chen†  †Department of Computer Science, Princeton University  ‡Institute for Interdisciplinary Information Sciences, Tsinghua University  {tianyug, danqic}@cs.princeton.edu  yxc18@mails.tsinghua.edu.cn<|md_end|>
<|box_start|>264 253 340 268<|box_end|><|ref_start|>title<|ref_end|><|md_start|># Abstract<|md_end|>
<|box_start|>147 284 461 694<|box_end|><|ref_start|>text<|ref_end|><|md_start|>This paper presents SimCSE, a simple contrastive learning framework that greatly advances state- of- the- art sentence embeddings. We first describe an unsupervised approach, which takes an input sentence and predicts itself in a contrastive objective, with only standard dropout used as noise. This simple method works surprisingly well, performing on par with previous supervised counterparts. We find that dropout acts as minimal data augmentation, and removing it leads to a representation collapse. Then, we propose a supervised approach, which incorporates annotated pairs from natural language inference datasets into our contrastive learning framework by using "entailment" pairs as positives and "contradiction" pairs as hard negatives. We evaluate SimCSE on standard semantic textual similarity (STS) tasks, and our unsupervised and supervised models using BERTbase achieve an average of \(76.3\%\) and \(81.6\%\) Spearman's correlation respectively, a \(4.2\%\) and \(2.2\%\) improvement compared to the previous best results. We also show both theoretically and empirically—that the contrastive learning objective regularizes pre- trained embeddings' anisotropic space to be more uniform, and it better aligns positive pairs when supervised signals are available.<|md_end|>
<|box_start|>119 710 259 725<|box_end|><|ref_start|>title<|ref_end|><|md_start|># 1 Introduction<|md_end|>
<|box_start|>118 737 488 848<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Learning universal sentence embeddings is a fundamental problem in natural language processing and has been studied extensively in the literature (Kiros et al., 2015; Hill et al., 2016; Conneau et al., 2017; Logeswaran and Lee, 2018; Cer et al., 2018; Reimers and Gurevych, 2019, inter alia). In this work, we advance state- of- the- art sentence <|txt_contd|><|md_end|>
<|box_start|>513 254 883 381<|box_end|><|ref_start|>text<|ref_end|><|md_start|>embedding methods and demonstrate that a contrastive objective can be extremely effective when coupled with pre- trained language models such as BERT (Devlin et al., 2019) or RoBERTa (Liu et al., 2019). We present SimCSE, a simple contrastive sentence embedding framework, which can produce superior sentence embeddings, from either unlabeled or labeled data.<|md_end|>
<|box_start|>513 385 883 675<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Our unsupervised SimCSE simply predicts the input sentence itself with only dropout (Srivastava et al., 2014) used as noise (Figure 1(a)). In other words, we pass the same sentence to the pre- trained encoder twice: by applying the standard dropout twice, we can obtain two different embeddings as "positive pairs". Then we take other sentences in the same mini- batch as "negatives", and the model predicts the positive one among the negatives. Although it may appear strikingly simple, this approach outperforms training objectives such as predicting next sentences (Logeswaran and Lee, 2018) and discrete data augmentation (e.g., word deletion and replacement) by a large margin, and even matches previous supervised methods. Through careful analysis, we find that dropout acts as minimal "data augmentation" of hidden representations while removing it leads to a representation collapse.<|md_end|>
<|box_start|>513 679 883 918<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Our supervised SimCSE builds upon the recent success of using natural language inference (NLI) datasets for sentence embeddings (Conneau et al., 2017; Reimers and Gurevych, 2019) and incorporates annotated sentence pairs in contrastive learning (Figure 1(b)). Unlike previous work that casts it as a 3- way classification task (entailment, neutral, and contradiction), we leverage the fact that entailment pairs can be naturally used as positive instances. We also find that adding corresponding contradiction pairs as hard negatives further improves performance. This simple use of NLI datasets achieves a substantial improvement compared to prior methods using the same datasets. We also compare to other labeled sentence- pair<|md_end|>
--------------------------------------------------
<|box_start|>123 087 878 259<|box_end|><|ref_start|>image<|ref_end|><|md_start|>![]('img_url')<|md_end|>
<|box_start|>116 270 884 313<|box_end|><|ref_start|>image_caption<|ref_end|><|md_start|>Figure 1: (a) Unsupervised SimCSE predicts the input sentence itself from in-batch negatives, with different hidden dropout masks applied. (b) Supervised SimCSE leverages the NLI datasets and takes the entailment (premise-hypothesis) pairs as positives, and contradiction pairs as well as other in-batch instances as negatives.<|md_end|>
<|box_start|>118 338 487 370<|box_end|><|ref_start|>text<|ref_end|><|md_start|>datasets and find that NLI datasets are especially effective for learning sentence embeddings.<|md_end|>
<|box_start|>118 371 488 693<|box_end|><|ref_start|>text<|ref_end|><|md_start|>To better understand the strong performance of SimCSE, we borrow the analysis tool from Wang and Isola (2020), which takes alignment between semantically- related positive pairs and uniformity of the whole representation space to measure the quality of learned embeddings. Through empirical analysis, we find that our unsupervised SimCSE essentially improves uniformity while avoiding degenerated alignment via dropout noise, thus improving the expressiveness of the representations. The same analysis shows that the NLI training signal can further improve alignment between positive pairs and produce better sentence embeddings. We also draw a connection to the recent findings that pre- trained word embeddings suffer from anisotropy (Ethayarajh, 2019; Li et al., 2020) and prove that—through a spectrum perspective—the contrastive learning objective "flattens" the singular value distribution of the sentence embedding space, hence improving uniformity.<|md_end|>
<|box_start|>118 695 488 919<|box_end|><|ref_start|>text<|ref_end|><|md_start|>We conduct a comprehensive evaluation of SimCSE on seven standard semantic textual similarity (STS) tasks (Agirre et al., 2012, 2013, 2014, 2015, 2016; Cer et al., 2017; Marelli et al., 2014) and seven transfer tasks (Conneau and Kiela, 2018). On the STS tasks, our unsupervised and supervised models achieve a \(76.3\%\) and \(81.6\%\) averaged Spearman's correlation respectively using \(\mathrm{BERT}_{\mathrm{base}}\), a \(4.2\%\) and \(2.2\%\) improvement compared to previous best results. We also achieve competitive performance on the transfer tasks. Finally, we identify an incoherent evaluation issue in the literature and consolidate the results of different settings for future work in evaluation of sentence embeddings.<|md_end|>
<|box_start|>514 337 849 354<|box_end|><|ref_start|>title<|ref_end|><|md_start|># 2 Background: Contrastive Learning<|md_end|>
<|box_start|>513 363 883 540<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Contrastive learning aims to learn effective representation by pulling semantically close neighbors together and pushing apart non- neighbors (Hadsell et al., 2006). It assumes a set of paired examples \(\mathcal{D} = \{(x_i,x_i^+)\}_{i = 1}^m\), where \(x_{i}\) and \(x_{i}^{+}\) are semantically related. We follow the contrastive framework in Chen et al. (2020) and take a cross- entropy objective with in- batch negatives (Chen et al., 2017; Henderson et al., 2017): let \(\mathbf{h}_i\) and \(\mathbf{h}_i^+\) denote the representations of \(x_{i}\) and \(x_{i}^{+}\), the training objective for \((x_{i},x_{i}^{+})\) with a mini- batch of \(N\) pairs is:<|md_end|>
<|box_start|>577 548 881 596<|box_end|><|ref_start|>equation<|ref_end|><|md_start|>\[
\ell_{i} = -\log \frac{e^{\sin(\mathbf{h}_{i},\mathbf{h}_{i}^{+}) / \tau}}{\sum_{j = 1}^{N}e^{\sin(\mathbf{h}_{i},\mathbf{h}_{j}^{+}) / \tau}}, \tag{1}
\]<|md_end|>
<|box_start|>513 604 884 720<|box_end|><|ref_start|>text<|ref_end|><|md_start|>where \(\tau\) is a temperature hyperparameter and \(\sin (\mathbf{h}_1,\mathbf{h}_2)\) is the cosine similarity \(\frac{\mathbf{h}_1^+\mathbf{h}_2}{||\mathbf{h}_1||\cdot||\mathbf{h}_2||}\). In this work, we encode input sentences using a pre- trained language model such as BERT (Devlin et al., 2019) or RoBERTa (Liu et al., 2019): \(\mathbf{h} = f_{\theta}(x)\), and then fine- tune all the parameters using the contrastive learning objective (Eq. 1).<|md_end|>
<|box_start|>513 727 884 919<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Positive instances. One critical question in contrastive learning is how to construct \((x_{i},x_{i}^{+})\) pairs. In visual representations, an effective solution is to take two random transformations of the same image (e.g., cropping, flipping, distortion and rotation) as \(x_{i}\) and \(x_{i}^{+}\) (Dosovitskiy et al., 2014). A similar approach has been recently adopted in language representations (Wu et al., 2020; Meng et al., 2021) by applying augmentation techniques such as word deletion, reordering, and substitution. However, data augmentation in NLP is inherently difficult because of its discrete nature. As we will see in §3,<|md_end|>
--------------------------------------------------
<|box_start|>116 085 488 116<|box_end|><|ref_start|>text<|ref_end|><|md_start|>simply using standard dropout on intermediate representations outperforms these discrete operators.<|md_end|>
<|box_start|>118 119 488 312<|box_end|><|ref_start|>text<|ref_end|><|md_start|>In NLP, a similar contrastive learning objective has been explored in different contexts (Henderson et al., 2017; Gillick et al., 2019; Karpukhin et al., 2020). In these cases, \((x_{i},x_{i}^{+})\) are collected from supervised datasets such as question- passage pairs. Because of the distinct nature of \(x_{i}\) and \(x_{i}^{+}\), these approaches always use a dual- encoder framework, i.e., using two independent encoders \(f_{\theta_1}\) and \(f_{\theta_2}\) for \(x_{i}\) and \(x_{i}^{+}\). For sentence embeddings, Logeswaran and Lee (2018) also use contrastive learning with a dual- encoder approach, by forming current sentence and next sentence as \((x_{i},x_{i}^{+})\).<|md_end|>
<|box_start|>118 318 489 447<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Alignment and uniformity. Recently, Wang and Isola (2020) identify two key properties related to contrastive learning—alignment and uniformity—and propose to use them to measure the quality of representations. Given a distribution of positive pairs \(p_{\mathrm{pos}}\), alignment calculates expected distance between embeddings of the paired instances (assuming representations are already normalized):<|md_end|>
<|box_start|>165 462 485 491<|box_end|><|ref_start|>equation<|ref_end|><|md_start|>\[
\ell_{\mathrm{align}}\triangleq \underset {(x,x^{+})\sim p_{\mathrm{pos}}}{\mathbb{E}}\| f(x) - f(x^{+})\|^{2}. \tag{2}
\]<|md_end|>
<|box_start|>116 500 487 530<|box_end|><|ref_start|>text<|ref_end|><|md_start|>On the other hand, uniformity measures how well the embeddings are uniformly distributed:<|md_end|>
<|box_start|>130 542 485 576<|box_end|><|ref_start|>equation<|ref_end|><|md_start|>\[
\ell_{\mathrm{uniform}}\triangleq \log \underset {x,y}{\mathbb{E}}\underset {i\sim d_{\mathrm{data}}}{\mathbb{E}}e^{-2\| f(x) - f(y)\| ^2}, \tag{3}
\]<|md_end|>
<|box_start|>116 587 488 701<|box_end|><|ref_start|>text<|ref_end|><|md_start|>where \(p_{\mathrm{data}}\) denotes the data distribution. These two metrics are well aligned with the objective of contrastive learning: positive instances should stay close and embeddings for random instances should scatter on the hypersphere. In the following sections, we will also use the two metrics to justify the inner workings of our approaches.<|md_end|>
<|box_start|>118 715 345 732<|box_end|><|ref_start|>title<|ref_end|><|md_start|># 3 Unsupervised SimCSE<|md_end|>
<|box_start|>116 743 488 919<|box_end|><|ref_start|>text<|ref_end|><|md_start|>The idea of unsupervised SimCSE is extremely simple: we take a collection of sentences \(\{x_{i}\}_{i = 1}^{m}\) and use \(x_{i}^{+} = x_{i}\). The key ingredient to get this to work with identical positive pairs is through the use of independently sampled dropout masks for \(x_{i}\) and \(x_{i}^{+}\). In standard training of Transformers (Vaswani et al., 2017), there are dropout masks placed on fully- connected layers as well as attention probabilities (default \(p = 0.1\)). We denote \(\mathbf{h}_{i}^{\tilde{z}} = f_{\theta}(x_{i},z)\) where \(z\) is a random mask for dropout. We simply feed the same input to the encoder twice and get <|txt_contd|><|md_end|>
<|box_start|>531 083 858 243<|box_end|><|ref_start|>table<|ref_end|><|md_start|><fcel>Data augmentation<lcel><lcel><lcel><fcel>STS-B<nl>
<fcel>None (unsup. SimCSE)<lcel><lcel><lcel><fcel>82.5<nl>
<fcel>Crop<fcel>10%<fcel>20%<fcel>30%<ecel><nl>
<ucel><fcel>77.8<fcel>71.4<fcel>63.6<ecel><nl>
<fcel>Word deletion<fcel>10%<fcel>20%<fcel>30%<ecel><nl>
<ucel><fcel>75.9<fcel>72.2<fcel>68.2<ecel><nl>
<fcel>Delete one word<ecel><ecel><fcel>75.9<ecel><nl>
<fcel>w/o dropout<ecel><ecel><fcel>74.2<ecel><nl>
<fcel>Synonym replacement<ecel><ecel><fcel>77.4<ecel><nl>
<fcel>MLM 15%<ecel><ecel><fcel>62.2<ecel><nl>
<|md_end|>
<|box_start|>512 258 884 344<|box_end|><|ref_start|>table_caption<|ref_end|><|md_start|>Table 1: Comparison of data augmentations on STS-B development set (Spearman's correlation). Crop \(k\%\) .. keep \(100 - k\%\) of the length; word deletion \(k\%\) : delete \(k\%\) words; Synonym replacement: use nlpaug (Ma, 2019) to randomly replace one word with its synonym; MLM \(k\%\) : use \(\mathrm{BERT}_{\mathrm{base}}\) to replace \(k\%\) of words.<|md_end|>
<|box_start|>543 358 846 438<|box_end|><|ref_start|>table<|ref_end|><|md_start|><fcel>Training objective<fcel>fθ<fcel>(fθ1, fθ2)<nl>
<fcel>Next sentence<fcel>67.1<fcel>68.9<nl>
<fcel>Next 3 sentences<fcel>67.4<fcel>68.8<nl>
<fcel>Delete one word<fcel>75.9<fcel>73.1<nl>
<fcel>Unsupervised SimCSE<fcel>82.5<fcel>80.7<nl>
<|md_end|>
<|box_start|>512 453 884 538<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Table 2: Comparison of different unsupervised objectives (STS- B development set, Spearman's correlation). The two columns denote whether we use one encoder or two independent encoders. Next 3 sentences: randomly sample one from the next 3 sentences. Delete one word: delete one word randomly (see Table 1).<|md_end|>
<|box_start|>512 565 883 595<|box_end|><|ref_start|>text<|ref_end|><|md_start|>two embeddings with different dropout masks \(z,z^{\prime}\) and the training objective of SimCSE becomes:<|md_end|>
<|box_start|>571 605 881 663<|box_end|><|ref_start|>equation<|ref_end|><|md_start|>\[
\ell_{i} = -\log \frac{e^{\mathrm{sim}(\mathbf{h}_{i}^{\tilde{z}_{i}},\mathbf{h}_{i}^{z_{i}^{\prime}}) / \tau}}{\sum_{j = 1}^{N}e^{\mathrm{sim}(\mathbf{h}_{i}^{\tilde{z}_{i}},\mathbf{h}_{j}^{z_{j}^{\prime}}) / \tau}}, \tag{4}
\]<|md_end|>
<|box_start|>512 673 883 721<|box_end|><|ref_start|>text<|ref_end|><|md_start|>for a mini- batch of \(N\) sentences. Note that \(z\) is just the standard dropout mask in Transformers and we do not add any additional dropout.<|md_end|>
<|box_start|>513 728 884 873<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Dropout noise as data augmentation. We view it as a minimal form of data augmentation: the positive pair takes exactly the same sentence, and their embeddings only differ in dropout masks. We compare this approach to other training objectives on the STS- B development set (Cer et al., 2017)2. Table 1 compares our approach to common data augmentation techniques such as crop, word deletion and replacement, which can be viewed as<|md_end|>
--------------------------------------------------
<|box_start|>116 171 489 229<|box_end|><|ref_start|>table_caption<|ref_end|><|md_start|>Table 3: Effects of different dropout probabilities \(p\) on the STS-B development set (Spearman's correlation, \(\mathrm{BERT}_{\mathrm{base}})\) .Fixed 0.1: default 0.1 dropout rate but apply the same dropout mask on both \(x_{i}\) and \(x_{i}^{+}\)<|md_end|>
<|box_start|>152 082 448 155<|box_end|><|ref_start|>table<|ref_end|><|md_start|><fcel>p<fcel>0.0<fcel>0.01<fcel>0.05<fcel>0.1<nl>
<fcel>STS-B<fcel>71.1<fcel>72.6<fcel>81.1<fcel>82.5<nl>
<fcel>p<fcel>0.15<fcel>0.2<fcel>0.5<fcel>Fixed 0.1<nl>
<fcel>STS-B<fcel>81.4<fcel>80.5<fcel>71.0<fcel>43.6<nl>
<|md_end|>
<|box_start|>116 253 488 317<|box_end|><|ref_start|>text<|ref_end|><|md_start|>\(\mathbf{h} = f_{\theta}(g(x),z)\) and \(g\) is a (random) discrete operator on \(x\) .We note that even deleting one word would hurt performance and none of the discrete augmentations outperforms dropout noise.<|md_end|>
<|box_start|>118 321 488 448<|box_end|><|ref_start|>text<|ref_end|><|md_start|>We also compare this self- prediction training objective to the next- sentence objective used in Logeswaran and Lee (2018), taking either one encoder or two independent encoders. As shown in Table 2, we find that SimCSE performs much better than the next- sentence objectives (82.5 vs 67.4 on STSB) and using one encoder instead of two makes a significant difference in our approach.<|md_end|>
<|box_start|>118 458 488 874<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Why does it work? To further understand the role of dropout noise in unsupervised SimCSE, we try out different dropout rates in Table 3 and observe that all the variants underperform the default dropout probability \(p = 0.1\) from Transformers. We find two extreme cases particularly interesting: "no dropout" \((p = 0)\) and "fixed 0.1" (using default dropout \(p = 0.1\) but the same dropout masks for the pair). In both cases, the resulting embeddings for the pair are exactly the same, and it leads to a dramatic performance degradation. We take the checkpoints of these models every 10 steps during training and visualize the alignment and uniformity metrics in Figure 2, along with a simple data augmentation model "delete one word". As clearly shown, starting from pre- trained checkpoints, all models greatly improve uniformity. However, the alignment of the two special variants also degrades drastically, while our unsupervised SimCSE keeps a steady alignment, thanks to the use of dropout noise. It also demonstrates that starting from a pretrained checkpoint is crucial for it provides good initial alignment. At last, "delete one word" improves the alignment yet achieves a smaller gain on the uniformity metric, and eventually underperforms unsupervised SimCSE.<|md_end|>
<|box_start|>517 084 874 251<|box_end|><|ref_start|>image<|ref_end|><|md_start|>![]('img_url')<|md_end|>
<|box_start|>512 261 883 332<|box_end|><|ref_start|>image_caption<|ref_end|><|md_start|>Figure 2: \(\ell_{\mathrm{align}} - \ell_{\mathrm{uniform}}\) plot for unsupervised SimCSE, "no dropout", "fixed 0.1", and "delete one word". We visualize checkpoints every 10 training steps and the arrows indicate the training direction. For both \(\ell_{\mathrm{align}}\) and \(\ell_{\mathrm{uniform}}\), lower numbers are better.<|md_end|>
<|box_start|>514 352 716 369<|box_end|><|ref_start|>title<|ref_end|><|md_start|># 4 Supervised SimCSE<|md_end|>
<|box_start|>513 382 883 622<|box_end|><|ref_start|>text<|ref_end|><|md_start|>We have demonstrated that adding dropout noise is able to keep a good alignment for positive pairs \((x,x^{+})\sim p_{\mathrm{pos}}\) . In this section, we study whether we can leverage supervised datasets to provide better training signals for improving alignment of our approach. Prior work (Conneau et al., 2017; Reimers and Gurevych, 2019) has demonstrated that supervised natural language inference (NLI) datasets (Bowman et al., 2015; Williams et al., 2018) are effective for learning sentence embeddings, by predicting whether the relationship between two sentences is entailment, neutral or contradiction. In our contrastive learning framework, we instead directly take \((x_{i},x_{i}^{+})\) pairs from supervised datasets and use them to optimize Eq. 1. <|md_end|>
<|box_start|>513 630 884 823<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Choices of labeled data. We first explore which supervised datasets are especially suitable for constructing positive pairs \((x_{i},x_{i}^{+})\) .We experiment with a number of datasets with sentence- pair examples, including 1) \(\mathrm{QQP^4}\) : Quora question pairs; 2) Flickr30k (Young et al., 2014): each image is annotated with 5 human- written captions and we consider any two captions of the same image as a positive pair; 3) ParaNMT (Wieting and Gimpel, 2018): a large- scale back- translation paraphrase dataset; and finally, 4) NLI datasets: SNLI (Bowman et al., 2015) and MNLI (Williams et al., 2018).<|md_end|>
<|box_start|>513 825 883 855<|box_end|><|ref_start|>text<|ref_end|><|md_start|>We train the contrastive learning model (Eq. 1) with different datasets and compare the results in<|md_end|>
--------------------------------------------------
<|box_start|>118 085 488 293<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Table 4. For a fair comparison, we also run experiments with the same # of training pairs. Among all the options, using entailment pairs from the NLI (SNLI + MNLI) datasets performs the best. We think this is reasonable, as the NLI datasets consist of high- quality and crowd- sourced pairs. Also, human annotators are expected to write the hypotheses manually based on the premises and two sentences tend to have less lexical overlap. For instance, we find that the lexical overlap (F1 measured between two bags of words) for the entailment pairs (SNLI + MNLI) is \(39\%\), while they are \(60\%\) and \(55\%\) for QQP and ParaNMT.<|md_end|>
<|box_start|>118 302 488 461<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Contradiction as hard negatives. Finally, we further take the advantage of the NLI datasets by using its contradiction pairs as hard negatives<sup>6</sup>. In NLI datasets, given one premise, annotators are required to manually write one sentence that is absolutely true (entailment), one that might be true (neutral), and one that is definitely false (contradiction). Therefore, for each premise and its entailment hypothesis, there is an accompanying contradiction hypothesis<sup>7</sup> (see Figure 1 for an example).<|md_end|>
<|box_start|>118 462 488 526<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Formally, we extend \((x_{i},x_{i}^{+})\) to \((x_{i},x_{i}^{+},x_{i}^{- })\) where \(x_{i}\) is the premise, \(x_{i}^{+}\) and \(x_{i}^{- }\) are entailment and contradiction hypotheses. The training objective \(\ell_{i}\) is then defined by \(N\) is mini- batch size):<|md_end|>
<|box_start|>127 539 484 600<|box_end|><|ref_start|>equation<|ref_end|><|md_start|>\[
-\log \frac{e^{\sin(\mathbf{h}_i,\mathbf{h}_j^+) / \tau}}{\sum_{j = 1}^{N}\left(e^{\sin(\mathbf{h}_i,\mathbf{h}_j^+) / \tau} + e^{\sin(\mathbf{h}_i,\mathbf{h}_j^-) / \tau}\right)}. \tag{5}
\]<|md_end|>
<|box_start|>118 603 488 729<|box_end|><|ref_start|>text<|ref_end|><|md_start|>As shown in Table 4, adding hard negatives can further improve performance \((84.9 \rightarrow 86.2)\) and this is our final supervised SimCSE. We also tried to add the ANLI dataset (Nie et al., 2020) or combine it with our unsupervised SimCSE approach, but didn't find a meaningful improvement. We also considered a dual encoder framework in supervised SimCSE and it hurt performance \((86.2 \rightarrow 84.2)\).<|md_end|>
<|box_start|>118 742 369 759<|box_end|><|ref_start|>title<|ref_end|><|md_start|># 5 Connection to Anisotropy<|md_end|>
<|box_start|>118 769 487 847<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Recent work identifies an anisotropy problem in language representations (Ethayarajh, 2019; Li et al., 2020), i.e., the learned embeddings occupy a narrow cone in the vector space, which severely limits their expressiveness. Gao et al. (2019) <|txt_contd|><|md_end|>
<|box_start|>549 083 842 280<|box_end|><|ref_start|>table<|ref_end|><|md_start|><fcel>Dataset<fcel>sample<fcel>full<nl>
<fcel>Unsup. SimCSE (1m)<fcel>-<fcel>82.5<nl>
<fcel>QQP (134k)<fcel>81.8<fcel>81.8<nl>
<fcel>Flickr30k (318k)<fcel>81.5<fcel>81.4<nl>
<fcel>ParaNMT (5m)<fcel>79.7<fcel>78.7<nl>
<fcel>SNLI+MNLI<ecel><ecel><nl>
<fcel>entailment (314k)<fcel>84.1<fcel>84.9<nl>
<fcel>neutral (314k)<fcel>82.6<fcel>82.9<nl>
<fcel>contradiction (314k)<fcel>77.5<fcel>77.6<nl>
<fcel>all (942k)<fcel>81.7<fcel>81.9<nl>
<fcel>SNLI+MNLI<ecel><ecel><nl>
<fcel>entailment + hard neg.<fcel>-<fcel>86.2<nl>
<fcel>+ ANLI (52k)<fcel>-<fcel>85.0<nl>
<|md_end|>
<|box_start|>513 296 883 424<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Table 4: Comparisons of different supervised datasets as positive pairs. Results are Spearman's correlations on the STS- B development set using \(\mathrm{BERT}_{\mathrm{base}}\) (we use the same hyperparameters as the final SimCSE model). Numbers in brackets denote the # of pairs. Sample: subsampling 134k positive pairs for a fair comparison among datasets; full: using the full dataset. In the last block, we use entailment pairs as positives and contradiction pairs as hard negatives (our final model).<|md_end|>
<|box_start|>513 451 883 579<|box_end|><|ref_start|>text<|ref_end|><|md_start|>demonstrate that language models trained with tied input/output embeddings lead to anisotropic word embeddings, and this is further observed by Ethayarajh (2019) in pre- trained contextual representations. Wang et al. (2020) show that singular values of the word embedding matrix in a language model decay drastically: except for a few dominating singular values, all others are close to zero.<|md_end|>
<|box_start|>513 580 884 740<|box_end|><|ref_start|>text<|ref_end|><|md_start|>A simple way to alleviate the problem is postprocessing, either to eliminate the dominant principal components (Arora et al., 2017; Mu and Viswanath, 2018), or to map embeddings to an isotropic distribution (Li et al., 2020; Su et al., 2021). Another common solution is to add regularization during training (Gao et al., 2019; Wang et al., 2020). In this work, we show that—both theoretically and empirically—the contrastive objective can also alleviate the anisotropy problem.<|md_end|>
<|box_start|>513 742 883 870<|box_end|><|ref_start|>text<|ref_end|><|md_start|>The anisotropy problem is naturally connected to uniformity (Wang and Isola, 2020), both highlighting that embeddings should be evenly distributed in the space. Intuitively, optimizing the contrastive learning objective can improve uniformity (or ease the anisotropy problem), as the objective pushes negative instances apart. Here, we take a singular spectrum perspective—which is a common practice<|md_end|>
--------------------------------------------------
<|box_start|>116 085 488 165<|box_end|><|ref_start|>text<|ref_end|><|md_start|>in analyzing word embeddings (Mu and Viswanath, 2018; Gao et al., 2019; Wang et al., 2020), and show that the contrastive objective can "flatten" the singular value distribution of sentence embeddings and make the representations more isotropic.<|md_end|>
<|box_start|>116 166 488 245<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Following Wang and Isola (2020), the asymptotics of the contrastive learning objective (Eq. 1) can be expressed by the following equation when the number of negative instances approaches infinity (assuming \(f(x)\) is normalized):<|md_end|>
<|box_start|>133 254 485 327<|box_end|><|ref_start|>equation<|ref_end|><|md_start|>\[
\begin{array}{rl} & {-\frac{1}{\tau}\underset {(x,x^{+})\sim p_{\mathrm{pos}}}{\mathbb{E}}\left[f(x)^{\top}f(x^{+})\right]}\\ & {+\underset {x\sim p_{\mathrm{data}}}{\mathbb{E}}\left[\log \underset {x^{-}\sim p_{\mathrm{data}}}{\mathbb{E}}\left[e^{f(x)^{\top}f(x^{-}) / \tau}\right]\right],} \end{array} \tag{6}
\]<|md_end|>
<|box_start|>116 336 488 416<|box_end|><|ref_start|>text<|ref_end|><|md_start|>where the first term keeps positive instances similar and the second pushes negative pairs apart. When \(p_{\mathrm{data}}\) is uniform over finite samples \(\{x_{i}\}_{i = 1}^{m}\), with \(\mathbf{h}_i = f(x_i)\), we can derive the following formula from the second term with Jensen's inequality:<|md_end|>
<|box_start|>138 424 485 557<|box_end|><|ref_start|>equation<|ref_end|><|md_start|>\[
\begin{array}{rl} & {\quad x\sim p_{\mathrm{data}}\left[\log \underset {x^{-}\sim p_{\mathrm{data}}}{\mathbb{E}}\left[e^{f(x)^{\top}f(x^{-}) / \tau}\right]\right]}\\ & {= \frac{1}{m}\sum_{i = 1}^{m}\log \left(\frac{1}{m}\sum_{j = 1}^{m}e^{\mathbf{h}_i}\mathbf{h}_j / \tau\right)}\\ & {\geq \frac{1}{\tau m^2}\sum_{i = 1}^{m}\sum_{j = 1}^{m}\mathbf{h}_i^{\top}\mathbf{h}_j.} \end{array} \tag{7}
\]<|md_end|>
<|box_start|>116 565 488 663<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Let \(\mathbf{W}\) be the sentence embedding matrix corresponding to \(\{x_{i}\}_{i = 1}^{m}\), i.e., the \(i\)- th row of \(\mathbf{W}\) is \(\mathbf{h}_i\). Optimizing the second term in Eq. 6 essentially minimizes an upper bound of the summation of all elements in \(\mathbf{W}\mathbf{W}^{\top}\), i.e., \(\mathrm{Sum}(\mathbf{W}\mathbf{W}^{\top}) = \sum_{i = 1}^{m}\sum_{j = 1}^{m}\mathbf{h}_i^{\top}\mathbf{h}_j\).<|md_end|>
<|box_start|>116 663 488 870<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Since we normalize \(\mathbf{h}_i\), all elements on the diagonal of \(\mathbf{W}\mathbf{W}^{\top}\) are 1 and then \(\mathrm{tr}(\mathbf{W}\mathbf{W}^{\top})\) (the sum of all eigenvalues) is a constant. According to Merikoski (1984), if all elements in \(\mathbf{W}\mathbf{W}^{\top}\) are positive, which is the case in most times according to Figure G.1, then \(\mathrm{Sum}(\mathbf{W}\mathbf{W}^{\top})\) is an upper bound for the largest eigenvalue of \(\mathbf{W}\mathbf{W}^{\top}\). When minimizing the second term in Eq. 6, we reduce the top eigenvalue of \(\mathbf{W}\mathbf{W}^{\top}\) and inherently "flatten" the singular spectrum of the embedding space. Therefore, contrastive learning is expected to alleviate the representation degeneration problem and improve uniformity of sentence embeddings.<|md_end|>
<|box_start|>116 873 489 919<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Compared to post- processing methods in Li et al. (2020); Su et al. (2021), which only aim to encourage isotropic representations, contrastive learning <|txt_contd|><|md_end|>
<|box_start|>513 085 882 132<|box_end|><|ref_start|>text<|ref_end|><|md_start|>also optimizes for aligning positive pairs by the first term in Eq. 6, which is the key to the success of SimCSE. A quantitative analysis is given in §7. <|md_end|>
<|box_start|>514 150 647 166<|box_end|><|ref_start|>title<|ref_end|><|md_start|># 6 Experiment<|md_end|>
<|box_start|>514 180 691 194<|box_end|><|ref_start|>title<|ref_end|><|md_start|># 6.1 Evaluation Setup<|md_end|>
<|box_start|>513 203 883 394<|box_end|><|ref_start|>text<|ref_end|><|md_start|>We conduct our experiments on 7 semantic textual similarity (STS) tasks. Note that all our STS experiments are fully unsupervised and no STS training sets are used. Even for supervised SimCSE, we simply mean that we take extra labeled datasets for training, following previous work (Conneau et al., 2017). We also evaluate 7 transfer learning tasks and provide detailed results in Appendix E. We share a similar sentiment with Reimers and Gurevych (2019) that the main goal of sentence embeddings is to cluster semantically similar sentences and hence take STS as the main result.<|md_end|>
<|box_start|>513 405 883 709<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Semantic textual similarity tasks. We evaluate on 7 STS tasks: STS 2012- 2016 (Agirre et al., 2012, 2013, 2014, 2015, 2016), STS Benchmark (Cer et al., 2017) and SICK- Relatedness (Marelli et al., 2014). When comparing to previous work, we identify invalid comparison patterns in published papers in the evaluation settings, including (a) whether to use an additional regressor, (b) Spearman's vs Pearson's correlation, and (c) how the results are aggregated (Table B.1). We discuss the detailed differences in Appendix B and choose to follow the setting of Reimers and Gurevych (2019) in our evaluation (no additional regressor, Spearman's correlation, and "all" aggregation). We also report our replicated study of previous work as well as our results evaluated in a different setting in Table B.2 and Table B.3. We call for unifying the setting in evaluating sentence embeddings for future research.<|md_end|>
<|box_start|>513 718 883 878<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Training details. We start from pre- trained checkpoints of BERT (Devlin et al., 2019) (uncased) or RoBERTa (Liu et al., 2019) (cased) and take the [CLS] representation as the sentence embedding (see §6.3 for comparison between different pooling methods). We train unsupervised SimCSE on \(10^{6}\) randomly sampled sentences from English Wikipedia, and train supervised SimCSE on the combination of MNLI and SNLI datasets (314k). More training details can be found in Appendix A.<|md_end|>
--------------------------------------------------
<|box_start|>146 082 853 454<|box_end|><|ref_start|>table<|ref_end|><|md_start|><fcel>Model<fcel>STS12<fcel>STS13<fcel>STS14<fcel>STS15<fcel>STS16<fcel>STS-B<fcel>SICK-R<fcel>Avg.<nl>
<fcel>Unsupervised models<lcel><lcel><lcel><lcel><lcel><lcel><lcel><lcel><nl>
<fcel>GloVe embeddings (avg.)▲<fcel>55.14<fcel>70.66<fcel>59.73<fcel>68.25<fcel>63.66<fcel>58.02<fcel>53.76<fcel>61.32<nl>
<fcel>BERTbase (first-last avg.)<fcel>39.70<fcel>59.38<fcel>49.67<fcel>66.03<fcel>66.19<fcel>53.87<fcel>62.06<fcel>56.70<nl>
<fcel>BERTbase-flow<fcel>58.40<fcel>67.10<fcel>60.85<fcel>75.16<fcel>71.22<fcel>68.66<fcel>64.47<fcel>66.55<nl>
<fcel>BERTbase-whitening<fcel>57.83<fcel>66.90<fcel>60.90<fcel>75.08<fcel>71.31<fcel>68.24<fcel>63.73<fcel>66.28<nl>
<fcel>IS-BERTbase<fcel>56.77<fcel>69.24<fcel>61.21<fcel>75.23<fcel>70.16<fcel>69.21<fcel>64.25<fcel>66.58<nl>
<fcel>CT-BERTbase<fcel>61.63<fcel>76.80<fcel>68.47<fcel>77.50<fcel>76.48<fcel>77.21<fcel>69.19<fcel>72.05<nl>
<fcel>* SimCSE-BERTbase<fcel>68.40<fcel>82.41<fcel>74.38<fcel>80.91<fcel>78.56<fcel>76.85<fcel>72.23<fcel>76.25<nl>
<fcel>RoBERTbase (first-last avg.)<fcel>40.88<fcel>58.74<fcel>49.07<fcel>65.63<fcel>61.48<fcel>58.55<fcel>61.63<fcel>56.57<nl>
<fcel>RoBERTbase-whitening<fcel>46.99<fcel>63.24<fcel>57.23<fcel>71.36<fcel>68.99<fcel>61.36<fcel>62.91<fcel>61.73<nl>
<fcel>DeCLUTR-RoBERTbase<fcel>52.41<fcel>73.19<fcel>65.32<fcel>77.12<fcel>78.63<fcel>72.41<fcel>68.62<fcel>69.99<nl>
<fcel>* SimCSE-RoBERTbase<fcel>70.16<fcel>81.77<fcel>73.24<fcel>81.36<fcel>80.65<fcel>80.22<fcel>68.56<fcel>76.57<nl>
<fcel>* SimCSE-RoBERTLarge<fcel>72.86<fcel>83.99<fcel>75.62<fcel>84.77<fcel>81.80<fcel>81.98<fcel>71.26<fcel>78.90<nl>
<fcel>Supervised models<lcel><lcel><lcel><lcel><lcel><lcel><lcel><lcel><nl>
<fcel>InferSent-GloVe▲<fcel>52.86<fcel>66.75<fcel>62.15<fcel>72.77<fcel>66.87<fcel>68.03<fcel>65.65<fcel>65.01<nl>
<fcel>Universal Sentence Encoder▲<fcel>64.49<fcel>67.80<fcel>64.61<fcel>76.83<fcel>73.18<fcel>74.92<fcel>76.69<fcel>71.22<nl>
<fcel>SBERTbase<fcel>70.97<fcel>76.53<fcel>73.19<fcel>79.09<fcel>74.30<fcel>77.03<fcel>72.91<fcel>74.89<nl>
<fcel>SBERTbase-flow<fcel>69.78<fcel>77.27<fcel>74.35<fcel>82.01<fcel>77.46<fcel>79.12<fcel>76.21<fcel>76.60<nl>
<fcel>SBERTbase-whitening<fcel>69.65<fcel>77.57<fcel>74.66<fcel>82.27<fcel>78.39<fcel>79.52<fcel>76.91<fcel>77.00<nl>
<fcel>CT-SBERTbase<fcel>74.84<fcel>83.20<fcel>78.07<fcel>83.84<fcel>77.93<fcel>81.46<fcel>76.42<fcel>79.39<nl>
<fcel>* SimCSE-BERTbase<fcel>75.30<fcel>84.67<fcel>80.19<fcel>85.40<fcel>80.82<fcel>84.25<fcel>80.39<fcel>81.57<nl>
<fcel>SRoBERTbase▲<fcel>71.54<fcel>72.49<fcel>70.80<fcel>78.74<fcel>73.69<fcel>77.77<fcel>74.46<fcel>74.21<nl>
<fcel>SRoBERTbase-whitening<fcel>70.46<fcel>77.07<fcel>74.46<fcel>81.64<fcel>76.43<fcel>79.49<fcel>76.65<fcel>76.60<nl>
<fcel>* SimCSE-RoBERTbase<fcel>76.53<fcel>85.21<fcel>80.95<fcel>86.03<fcel>82.57<fcel>85.83<fcel>80.50<fcel>82.52<nl>
<fcel>* SimCSE-RoBERTLarge<fcel>77.46<fcel>87.27<fcel>82.36<fcel>86.66<fcel>83.93<fcel>86.70<fcel>81.95<fcel>83.76<nl>
<|md_end|>
<|box_start|>118 470 884 527<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Table 5: Sentence embedding performance on STS tasks (Spearman's correlation, "all" setting). We highlight the highest numbers among models with the same pre- trained encoder. \(\clubsuit\) results from Reimers and Gurevych (2019); \(\mathfrak{S}\) : results from Zhang et al. (2020); all other results are reproduced or reevaluated by ourselves. For BERT- flow (Li et al., 2020) and whitening (Su et al., 2021), we only report the "NLI" setting (see Table C.1).<|md_end|>
<|box_start|>118 548 263 562<|box_end|><|ref_start|>title<|ref_end|><|md_start|># 6.2 Main Results<|md_end|>
<|box_start|>118 572 488 827<|box_end|><|ref_start|>text<|ref_end|><|md_start|>We compare unsupervised and supervised SimCSE to previous state- of- the- art sentence embedding methods on STS tasks. Unsupervised baselines include average GloVe embeddings (Pennington et al., 2014), average BERT or RoBERTa embeddings10, and post- processing methods such as BERT- flow (Li et al., 2020) and BERT- whitening (Su et al., 2021). We also compare to several recent methods using a contrastive objective, including 1) IS- BERT (Zhang et al., 2020), which maximizes the agreement between global and local features; 2) DeCLUTR (Giorgi et al., 2021), which takes different spans from the same document as positive pairs; 3) CT (Carlsson et al., 2021), which aligns embeddings of the same sentence from two different encoders.11 Other supervised <|txt_contd|><|md_end|>
<|box_start|>513 548 883 643<|box_end|><|ref_start|>text<|ref_end|><|md_start|>methods include InferSent (Conneau et al., 2017), Universal Sentence Encoder (Cer et al., 2018), and SBERT/SRoBERTa (Reimers and Gurevych, 2019) with post- processing methods (BERT- flow, whitening, and CT). We provide more details of these baselines in Appendix C.<|md_end|>
<|box_start|>513 645 884 854<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Table 5 shows the evaluation results on 7 STS tasks. SimCSE can substantially improve results on all the datasets with or without extra NLI supervision, greatly outperforming the previous state- of- the- art models. Specifically, our unsupervised SimCSE- BERTbase improves the previous best averaged Spearman's correlation from \(72.05\%\) to \(76.25\%\) , even comparable to supervised baselines. When using NLI datasets, SimCSE- BERTbase further pushes the state- of- the- art results to \(81.57\%\) The gains are more pronounced on RoBERTa encoders, and our supervised SimCSE achieves \(83.76\%\) with RoBERTalarge.<|md_end|>
<|box_start|>513 857 882 918<|box_end|><|ref_start|>text<|ref_end|><|md_start|>In Appendix E, we show that SimCSE also achieves on par or better transfer task performance compared to existing work, and an auxiliary MLM objective can further boost performance.<|md_end|>
--------------------------------------------------
<|box_start|>116 203 489 276<|box_end|><|ref_start|>table_caption<|ref_end|><|md_start|>Table 6: Ablation studies of different pooling methods in unsupervised and supervised SimCSE. \([CLS]w/\) MLP (train): using MLP on [CLS] during training but removing it during testing. The results are based on the development set of STS-B using \(\mathrm{BERT}_{\mathrm{base}}\)<|md_end|>
<|box_start|>152 082 448 188<|box_end|><|ref_start|>table<|ref_end|><|md_start|><fcel>Pooler<fcel>Unsup.<fcel>Sup.<nl>
<fcel>[CLS]<ecel><ecel><nl>
<fcel>w/ MLP<fcel>81.7<fcel>86.2<nl>
<fcel>w/ MLP (train)<fcel>82.5<fcel>85.8<nl>
<fcel>w/o MLP<fcel>80.9<fcel>86.2<nl>
<fcel>First-last avg.<fcel>81.2<fcel>86.1<nl>
<|md_end|>
<|box_start|>116 385 487 413<|box_end|><|ref_start|>table_caption<|ref_end|><|md_start|>Table 7: STS-B development results with different hard negative policies."N/A": no hard negative.<|md_end|>
<|box_start|>116 293 484 370<|box_end|><|ref_start|>table<|ref_end|><|md_start|><fcel>Hard neg<fcel>N/A<fcel>Contradiction<lcel><lcel><fcel>Contra.+ Neutral<nl>
<fcel>α<fcel>-<fcel>0.5<fcel>1.0<fcel>2.0<fcel>1.0<nl>
<fcel>STS-B<fcel>84.9<fcel>86.1<fcel>86.2<fcel>86.2<fcel>85.3<nl>
<|md_end|>
<|box_start|>118 443 291 458<|box_end|><|ref_start|>title<|ref_end|><|md_start|># 6.3 Ablation Studies<|md_end|>
<|box_start|>118 469 488 548<|box_end|><|ref_start|>text<|ref_end|><|md_start|>We investigate the impact of different pooling methods and hard negatives. All reported results in this section are based on the STS- B development set. We provide more ablation studies (normalization, temperature, and MLM objectives) in Appendix D.<|md_end|>
<|box_start|>118 557 488 846<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Pooling methods. Reimers and Gurevych (2019); Li et al. (2020) show that taking the average embeddings of pre- trained models (especially from both the first and last layers) leads to better performance than [CLS]. Table 6 shows the comparison between different pooling methods in both unsupervised and supervised SimCSE. For [CLS] representation, the original BERT implementation takes an extra MLP layer on top of it. Here, we consider three different settings for [CLS]: 1) keeping the MLP layer; 2) no MLP layer; 3) keeping MLP during training but removing it at testing time. We find that for unsupervised SimCSE, taking [CLS] representation with MLP only during training works the best; for supervised SimCSE, different pooling methods do not matter much. By default, we take [CLS] with MLP (train) for unsupervised SimCSE and [CLS] with MLP for supervised SimCSE.<|md_end|>
<|box_start|>116 856 488 919<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Hard negatives. Intuitively, it may be beneficial to differentiate hard negatives (contradiction examples) from other in- batch negatives. Therefore, we extend our training objective defined in Eq. 5 to <|txt_contd|><|md_end|>
<|box_start|>513 085 842 101<|box_end|><|ref_start|>text<|ref_end|><|md_start|>incorporate weighting of different negatives:<|md_end|>
<|box_start|>517 110 881 155<|box_end|><|ref_start|>equation<|ref_end|><|md_start|>\[
-\log \frac{e^{\sin(\mathbf{h}_i,\mathbf{h}_i^+) / \tau}}{\sum_{j = 1}^{N}\left(e^{\sin(\mathbf{h}_i,\mathbf{h}_j^+) / \tau} + \alpha^{\mathbb{1}^j}e^{\sin(\mathbf{h}_i,\mathbf{h}_j^-) / \tau}\right)}, \tag{8}
\]<|md_end|>
<|box_start|>513 169 883 281<|box_end|><|ref_start|>text<|ref_end|><|md_start|>where \(\mathbb{1}_i^j\in \{0,1\}\) is an indicator that equals 1 if and only if \(i = j\) .We train SimCSE with different values of \(\alpha\) and evaluate the trained models on the development set of STS- B.We also consider taking neutral hypotheses as hard negatives. As shown in Table 7, \(\alpha = 1\) performs the best, and neutral hypotheses do not bring further gains.<|md_end|>
<|box_start|>513 294 618 310<|box_end|><|ref_start|>title<|ref_end|><|md_start|># 7Analysis<|md_end|>
<|box_start|>513 321 883 352<|box_end|><|ref_start|>text<|ref_end|><|md_start|>In this section, we conduct further analyses to understand the inner workings of SimCSE.<|md_end|>
<|box_start|>513 360 884 696<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Uniformity and alignment. Figure 3 shows uniformity and alignment of different sentence embedding models along with their averaged STS results. In general, models which have both better alignment and uniformity achieve better performance, confirming the findings in Wang and Isola (2020). We also observe that (1) though pre- trained embeddings have good alignment, their uniformity is poor (i.e., the embeddings are highly anisotropic); (2) post- processing methods like BERT- flow and BERT- whitening greatly improve uniformity but also suffer a degeneration in alignment; (3) unsupervised SimCSE effectively improves uniformity of pre- trained embeddings whereas keeping a good alignment; (4) incorporating supervised data in SimCSE further amends alignment. In Appendix F, we further show that SimCSE can effectively flatten singular value distribution of pre- trained embeddings. In Appendix G, we demonstrate that SimCSE provides more distinguishable cosine similarities between different sentence pairs.<|md_end|>
<|box_start|>513 705 883 833<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Qualitative comparison. We conduct a smallscale retrieval experiment using \(\mathrm{SBERT}_{\mathrm{base}}\) and \(\mathrm{SimCSE - BERT}_{\mathrm{base}}\) .We use \(150\mathrm{k}\) captions from Flickr30k dataset and take any random sentence as query to retrieve similar sentences (based on cosine similarity). As several examples shown in Table 8, the retrieved sentences by SimCSE have a higher quality compared to those retrieved by SBERT.<|md_end|>
<|box_start|>513 845 664 861<|box_end|><|ref_start|>title<|ref_end|><|md_start|># 8 Related Work<|md_end|>
<|box_start|>513 873 882 918<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Early work in sentence embeddings builds upon the distributional hypothesis by predicting surrounding sentences of a given one (Kiros et al., 2015; Hill<|md_end|>
--------------------------------------------------
<|box_start|>127 082 872 238<|box_end|><|ref_start|>table<|ref_end|><|md_start|><fcel>SBERT_base<lcel><fcel>Supervised SimCSE-BERT_base<nl>
<fcel>Query: A man riding a small boat in a harbor.<lcel><lcel><nl>
<fcel>#1<fcel>A group of men traveling over the ocean in a small boat.<fcel>A man on a moored blue and white boat.<nl>
<fcel>#2<fcel>Two men sit on the bow of a colorful boat.<fcel>A man is riding in a boat on the water.<nl>
<fcel>#3<fcel>A man wearing a life jacket is in a small boat on a lake.<fcel>A man in a blue boat on the water.<nl>
<fcel>Query: A dog runs on the green grass near a wooden fence.<lcel><lcel><nl>
<fcel>#1<fcel>A dog runs on the green grass near a grove of trees.<fcel>The dog by the fence is running on the grass.<nl>
<fcel>#2<fcel>A brown and white dog runs through the green grass.<fcel>Dog running through grass in fenced area.<nl>
<fcel>#3<fcel>The dogs run in the green field.<fcel>A dog runs on the green grass near a grove of trees.<nl>
<|md_end|>
<|box_start|>146 248 851 263<|box_end|><|ref_start|>table_footnote<|ref_end|><|md_start|>Table 8: Retrieved top-3 examples by SBERT and supervised SimCSE from Flickr30k (150k sentences).<|md_end|>
<|box_start|>122 283 477 462<|box_end|><|ref_start|>image<|ref_end|><|md_start|>![]('img_url')<|md_end|>
<|box_start|>115 476 488 532<|box_end|><|ref_start|>image_caption<|ref_end|><|md_start|>Figure 3: \(\ell_{\mathrm{align}} - \ell_{\mathrm{uniform}}\) plot of models based on \(\mathrm{SBRT}_{\mathrm{base}}\). Color of points and numbers in brackets represent average STS performance (Spearman's correlation). Next3Sent: "next 3 sentences" from Table 2.<|md_end|>
<|box_start|>118 562 488 785<|box_end|><|ref_start|>text<|ref_end|><|md_start|>et al., 2016; Logeswaran and Lee, 2018). Pagliardini et al. (2018) show that simply augmenting the idea of word2vec (Mikolov et al., 2013) with n- gram embeddings leads to strong results. Several recent (and concurrent) approaches adopt contrastive objectives (Zhang et al., 2020; Giorgi et al., 2021; Wu et al., 2020; Meng et al., 2021; Carlsson et al., 2021; Kim et al., 2021; Yan et al., 2021) by taking different views—from data augmentation or different copies of models—of the same sentence or document. Compared to these work, SimCSE uses the simplest idea by taking different outputs of the same sentence from standard dropout, and performs the best on STS tasks.<|md_end|>
<|box_start|>118 792 488 919<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Supervised sentence embeddings are promised to have stronger performance compared to unsupervised counterparts. Conneau et al. (2017) propose to fine- tune a Siamese model on NLI datasets, which is further extended to other encoders or pre- trained models (Cer et al., 2018; Reimers and Gurevych, 2019). Furthermore, Wieting and Gimpel (2018); Wieting et al. (2020) demonstrate that <|txt_contd|><|md_end|>
<|box_start|>513 283 884 394<|box_end|><|ref_start|>text<|ref_end|><|md_start|>bilingual and back- translation corpora provide useful supervision for learning semantic similarity. Another line of work focuses on regularizing embeddings (Li et al., 2020; Su et al., 2021; Huang et al., 2021) to alleviate the representation degeneration problem (as discussed in §5), and yields substantial improvement over pre- trained language models.<|md_end|>
<|box_start|>513 406 641 423<|box_end|><|ref_start|>title<|ref_end|><|md_start|># 9 Conclusion<|md_end|>
<|box_start|>513 433 884 673<|box_end|><|ref_start|>text<|ref_end|><|md_start|>In this work, we propose SimCSE, a simple contrastive learning framework, which greatly improves state- of- the- art sentence embeddings on semantic textual similarity tasks. We present an unsupervised approach which predicts input sentence itself with dropout noise and a supervised approach utilizing NLI datasets. We further justify the inner workings of our approach by analyzing alignment and uniformity of SimCSE along with other baseline models. We believe that our contrastive objective, especially the unsupervised one, may have a broader application in NLP. It provides a new perspective on data augmentation with text input, and can be extended to other continuous representations and integrated in language model pre- training.<|md_end|>
<|box_start|>515 685 681 702<|box_end|><|ref_start|>title<|ref_end|><|md_start|># Acknowledgements<|md_end|>
<|box_start|>513 711 883 807<|box_end|><|ref_start|>text<|ref_end|><|md_start|>We thank Tao Lei, Jason Lee, Zhengyan Zhang, Jinhyuk Lee, Alexander Wettig, Zexuan Zhong, and the members of the Princeton NLP group for helpful discussion and valuable feedback. This research is supported by a Graduate Fellowship at Princeton University and a gift award from Apple.<|md_end|>
--------------------------------------------------
<|box_start|>118 085 213 100<|box_end|><|ref_start|>title<|ref_end|><|md_start|># References<|md_end|>
<|box_start|>116 108 488 224<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor Gonzalez- Agirre, Weiwei Guo, Inigo Lopez- Gazpio, Montse Maritxalar, Rada Mihalcea, German Rigau, Larraitz Uria, and Janyce Wiebe. 2015. SemEval- 2015 task 2: Semantic textual similarity, English, Spanish and pilot on interpretability. In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 252- 263. <|md_end|>
<|box_start|>118 235 487 326<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor Gonzalez- Agirre, Weiwei Guo, Rada Mihalcea, German Rigau, and Janyce Wiebe. 2014. SemEval- 2014 task 10: Multilingual semantic textual similarity. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 81- 91. <|md_end|>
<|box_start|>118 337 487 440<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Eneko Agirre, Carmen Banea, Daniel Cer, Mona Diab, Aitor Gonzalez- Agirre, Rada Mihalcea, German Rigau, and Janyce Wiebe. 2016. SemEval- 2016 task 1: Semantic textual similarity, monolingual and cross- lingual evaluation. In Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval- 2016), pages 497- 511. Association for Computational Linguistics.<|md_end|>
<|box_start|>118 451 487 567<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez- Agirre. 2012. SemEval- 2012 task 6: A pilot on semantic textual similarity. In *SEM 2012: The First Joint Conference on Lexical and Computational Semantics - Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012), pages 385- 393. <|md_end|>
<|box_start|>118 578 487 669<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez- Agirre, and Weiwei Guo. 2013. *SEM 2013 shared task: Semantic textual similarity. In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity, pages 32- 43. <|md_end|>
<|box_start|>116 680 488 731<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Sanjeev Arora, Yingyu Liang, and Tengyu Ma. 2017. A simple but tough- to- beat baseline for sentence embeddings. In International Conference on Learning Representations (ICLR).<|md_end|>
<|box_start|>118 741 488 807<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large annotated corpus for learning natural language inference. In Empirical Methods in Natural Language Processing (EMNLP), pages 632- 642. <|md_end|>
<|box_start|>118 816 487 881<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Fredrik Carlsson, Amaru Cuba Gyllensten, Evangelia Gogoulou, Erik Ylipa Helloworld, and Magnus Sahlgren. 2021. Semantic retuning with contrastive tension. In International Conference on Learning Representations (ICLR).<|md_end|>
<|box_start|>118 893 487 918<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez- Gazpio, and Lucia Specia. 2017. SemEval- 2017<|md_end|>
<|box_start|>531 087 883 139<|box_end|><|ref_start|>text<|ref_end|><|md_start|>task 1: Semantic textual similarity multilingual and crosslingual focused evaluation. In Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval- 2017), pages 1- 14. <|md_end|>
<|box_start|>513 149 883 240<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Daniel Cer, Yinfei Yang, Sheng- yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St. John, Noah Constant, Mario Guajardo- Cespedes, Steve Yuan, Chris Tar, Brian Strope, and Ray Kurzweil. 2018. Universal sentence encoder for English. In Empirical Methods in Natural Language Processing (EMNLP): System Demonstrations, pages 169- 174. <|md_end|>
<|box_start|>513 250 883 316<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. A simple framework for contrastive learning of visual representations. In International Conference on Machine Learning (ICML), pages 1597- 1607. <|md_end|>
<|box_start|>513 326 883 390<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Ting Chen, Yizhou Sun, Yue Shi, and Liangjie Hong. 2017. On sampling strategies for neural network- based collaborative filtering. In ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 767- 776. <|md_end|>
<|box_start|>513 401 883 453<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Alexis Conneau and Douwe Kiela. 2018. SentEval: An evaluation toolkit for universal sentence representations. In International Conference on Language Resources and Evaluation (LREC).<|md_end|>
<|box_start|>514 463 883 542<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Alexis Conneau, Douwe Kiela, Holger Schwenk, Loic Barrault, and Antoine Bordes. 2017. Supervised learning of universal sentence representations from natural language inference data. In Empirical Methods in Natural Language Processing (EMNLP), pages 670- 680. <|md_end|>
<|box_start|>513 552 883 642<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Jacob Devlin, Ming- Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre- training of deep bidirectional transformers for language understanding. In North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL- HLT), pages 4171- 4186. <|md_end|>
<|box_start|>513 652 883 705<|box_end|><|ref_start|>text<|ref_end|><|md_start|>William B. Dolan and Chris Brockett. 2005. Automatically constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005).<|md_end|>
<|box_start|>513 715 883 781<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Alexey Dosovitskiy, Jost Tobias Springenberg, Martin Riedmiller, and Thomas Brox. 2014. Discriminative unsupervised feature learning with convolutional neural networks. In Advances in Neural Information Processing Systems (NIPS), volume 27. <|md_end|>
<|box_start|>513 791 883 869<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Kawin Ethayarajh. 2019. How contextual are contextualized word representations? comparing the geometry of BERT, ELMo, and GPT- 2 embeddings. In Empirical Methods in Natural Language Processing and International Joint Conference on Natural Language Processing (EMNLP- IJCNLP), pages 55- 65. <|md_end|>
<|box_start|>513 880 883 918<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Jun Gao, Di He, Xu Tan, Tao Qin, Liwei Wang, and Tieyan Liu. 2019. Representation degeneration problem in training natural language generation<|md_end|>
--------------------------------------------------
<|box_start|>134 086 486 112<|box_end|><|ref_start|>text<|ref_end|><|md_start|>models. In International Conference on Learning Representations (ICLR).<|md_end|>
<|box_start|>118 124 487 189<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Dan Gillick, Sayali Kulkarni, Larry Lansing, Alessandro Presta, Jason Baldridge, Eugene Ie, and Diego Garcia- Olano. 2019. Learning dense representations for entity retrieval. In Computational Natural Language Learning (CoNLL), pages 528- 537. <|md_end|>
<|box_start|>118 201 487 279<|box_end|><|ref_start|>text<|ref_end|><|md_start|>John Giorgi, Osvald Nitski, Bo Wang, and Gary Bader. 2021. DeCLUTR: Deep contrastive learning for unsupervised textual representations. In Association for Computational Linguistics and International Joint Conference on Natural Language Processing (ACL- IJCNLP), pages 879- 895. <|md_end|>
<|box_start|>118 291 487 356<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Raia Hadsell, Sumit Chopra, and Yann LeCun. 2006. Dimensionality reduction by learning an invariant mapping. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), volume 2, pages 1735- 1742. IEEE.<|md_end|>
<|box_start|>118 367 487 432<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Matthew Henderson, Rami Al- Fofu, Brian Strope, Yun- Hsuan Sung, Laszlo Lukacs, Ruiqi Guo, Sanjiv Kumar, Balint Miklos, and Ray Kurzweil. 2017. Efficient natural language response suggestion for smart reply. arXiv preprint arXiv:1705.00652. <|md_end|>
<|box_start|>118 443 487 522<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Felix Hill, Kyunghyun Cho, and Anna Korhonen. 2016. Learning distributed representations of sentences from unlabelled data. In North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL- HLT), pages 1367- 1377. <|md_end|>
<|box_start|>118 534 487 586<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In ACM SIGKDD international conference on Knowledge discovery and data mining.<|md_end|>
<|box_start|>118 597 487 662<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Junjie Huang, Duyu Tang, Wanjun Zhong, Shuai Lu, Linjun Shou, Ming Gong, Daxin Jiang, and Nan Duan. 2021. Whiteningbert: An easy unsupervised sentence embedding approach. arXiv preprint arXiv:2104.01767. <|md_end|>
<|box_start|>118 674 487 752<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edimov, Danqi Chen, and Wen- tau Yih. 2020. Dense passage retrieval for open- domain question answering. In Empirical Methods in Natural Language Processing (EMNLP), pages 6769- 6781. <|md_end|>
<|box_start|>118 763 487 841<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Taeuk Kim, Kang Min Yoo, and Sang- goo Lee. 2021. Self- guided contrastive learning for BERT sentence representations. In Association for Computational Linguistics and International Joint Conference on Natural Language Processing (ACL- IJCNLP), pages 2528- 2540. <|md_end|>
<|box_start|>118 853 487 918<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov, Richard S Zemel, Antonio Torralba, Raquel Urtasun, and Sanja Fidler. 2015. Skip- thought vectors. In Advances in Neural Information Processing Systems (NIPS), pages 3294- 3302. <|md_end|>
<|box_start|>513 086 883 151<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Bohan Li, Hao Zhou, Junxian He, Mingxuan Wang, Yiming Yang, and Lei Li. 2020. On the sentence embeddings from pre- trained language models. In Empirical Methods in Natural Language Processing (EMNLP), pages 9119- 9130. <|md_end|>
<|box_start|>513 163 883 229<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692. <|md_end|>
<|box_start|>513 240 883 292<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Lajanugen Logeswarta and Honglak Lee. 2018. An efficient framework for learning sentence representations. In International Conference on Learning Representations (ICLR).<|md_end|>
<|box_start|>513 304 883 330<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Edward Ma. 2019. Nlp augmentation. https://github.com/makcedward/nlpaug.<|md_end|>
<|box_start|>513 341 883 420<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Marco Marelli, Stefano Menini, Marco Baroni, Luisa Bentivogli, Raffaella Bernardi, and Roberto Zamparelli. 2014. A SICK cure for the evaluation of compositional distributional semantic models. In International Conference on Language Resources and Evaluation (LREC), pages 216- 223. <|md_end|>
<|box_start|>513 432 883 496<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Yu Meng, Chenyan Xiong, Payal Bajaj, Saurabh Tiwary, Paul Bennett, Jiawei Han, and Xia Song. 2021. COCO- LM: Correcting and contrasting text sequences for language model pretraining. arXiv preprint arXiv:2102.08473. <|md_end|>
<|box_start|>513 508 882 548<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Jorma Kaarlo Meriokski. 1984. On the trace and the sum of elements of a matrix. Linear Algebra and its Applications, 60:177- 185. <|md_end|>
<|box_start|>513 559 883 624<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Tomas Mikolov, Ilya Sutskever, Kai Chen, G. Corrado, and J. Dean. 2013. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems (NIPS).<|md_end|>
<|box_start|>513 636 882 688<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Jiaqi Mu and Pramod Viswanath. 2018. All- but- the- top: Simple and effective postprocessing for word representations. In International Conference on Learning Representations (ICLR).<|md_end|>
<|box_start|>513 699 883 766<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. 2020. Adversarial NLI: A new benchmark for natural language understanding. In Association for Computational Linguistics (ACL), pages 4885- 4901. <|md_end|>
<|box_start|>513 777 883 855<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Matteo Pagliardini, Prakhar Gupta, and Martin Jaggi. 2018. Unsupervised learning of sentence embeddings using compositional n- gram features. In North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL- HLT), pages 528- 540. <|md_end|>
<|box_start|>513 866 883 919<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Bo Pang and Lillian Lee. 2004. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In Association for Computational Linguistics (ACL), pages 271- 278. <|md_end|>
--------------------------------------------------
<|box_start|>116 086 488 139<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In Association for Computational Linguistics (ACL), pages 115- 124. <|md_end|>
<|box_start|>116 151 488 217<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. GloVe: Global vectors for word representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532- 1543. <|md_end|>
<|box_start|>116 230 489 281<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Nils Reimers, Philip Beyer, and Iryna Gurevych. 2016. Task- oriented intrinsic evaluation of semantic textual similarity. In International Conference on Computational Linguistics (COLANG), pages 87- 96. <|md_end|>
<|box_start|>116 295 488 373<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Nils Reimers and Iryna Gurevych. 2019. SentenceBERT: Sentence embeddings using Siamese BERT networks. In Empirical Methods in Natural Language Processing and International Joint Conference on Natural Language Processing (EMNLP- IJCNLP), pages 3982- 3992. <|md_end|>
<|box_start|>116 385 488 463<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Empirical Methods in Natural Language Processing (EMNLP), pages 1631- 1642. <|md_end|>
<|box_start|>116 477 488 542<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: a simple way to prevent neural networks from overfitting. The Journal of Machine Learning Research (JMLR), 15(1):1929- 1958. <|md_end|>
<|box_start|>116 554 488 606<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Jianlin Su, Jiarun Cao, Weijie Liu, and Yangyiwen Ou. 2021. Whitening sentence representations for better semantics and faster retrieval. arXiv preprint arXiv:2103.15316. <|md_end|>
<|box_start|>116 619 488 685<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems (NIPS), pages 6000- 6010. <|md_end|>
<|box_start|>116 698 488 762<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Ellen M Voorhees and Dawn M Tice. 2000. Building a question answering test collection. In the 23rd annual international ACM SIGIR conference on Research and development in information retrieval, pages 200- 207. <|md_end|>
<|box_start|>116 775 488 841<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Lingxiao Wang, Jing Huang, Kevin Huang, Ziniu Hu, Guangtao Wang, and Quanquan Gu. 2020. Improving neural language generation with spectrum control. In International Conference on Learning Representations (ICLR).<|md_end|>
<|box_start|>116 853 488 918<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Tongzhou Wang and Phillip Isola. 2020. Understanding contrastive representation learning through alignment and uniformity on the hypersphere. In International Conference on Machine Learning (ICML), pages 9929- 9939. <|md_end|>
<|box_start|>512 086 884 139<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005. Annotating expressions of opinions and emotions in language. Language resources and evaluation, 39(2- 3):165- 210. <|md_end|>
<|box_start|>512 150 884 215<|box_end|><|ref_start|>text<|ref_end|><|md_start|>John Wieting and Kevin Gimpel. 2018. ParaNMT- 50M: Pushing the limits of paraphrastic sentence embeddings with millions of machine translations. In Association for Computational Linguistics (ACL), pages 451- 462. <|md_end|>
<|box_start|>512 225 883 291<|box_end|><|ref_start|>text<|ref_end|><|md_start|>John Wieting, Graham Neubig, and Taylor Berg- Kirkpatrick. 2020. A bilingual generative transformer for semantic sentence embedding. In Empirical Methods in Natural Language Processing (EMNLP), pages 1581- 1594. <|md_end|>
<|box_start|>513 301 883 380<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Adina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad- coverage challenge corpus for sentence understanding through inference. In North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL- HLT), pages 1112- 1122. <|md_end|>
<|box_start|>513 389 883 532<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jemison, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Transformers: State- of- the- art natural language processing. In Empirical Methods in Natural Language Processing (EMNLP): System Demonstrations, pages 38- 45. <|md_end|>
<|box_start|>513 544 883 595<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Zhuofeng Wu, Sinong Wang, Jiatao Gu, Madian Khabsa, Fei Sun, and Hao Ma. 2020. Clear: Contrastive learning for sentence representation. arXiv preprint arXiv:2012.15466. <|md_end|>
<|box_start|>513 606 883 698<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Yuanmeng Yan, Rumen Li, Sirui Wang, Fuzheng Zhang, Wei Wu, and Wexran Xu. 2021. ConSERT: A contrastive framework for self- supervised sentence representation transfer. In Association for Computational Linguistics and International Joint Conference on Natural Language Processing (ACL- IJCNLP), pages 5065- 5075. <|md_end|>
<|box_start|>512 708 883 774<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Peter Young, Alice Lai, Micah Hodosh, and Julia Hock- enmaier. 2014. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics, 2:67- 78. <|md_end|>
<|box_start|>513 783 883 850<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Yan Zhang, Ruidan He, Zuozhu Liu, Kwan Hui Lim, and Lidong Bing. 2020. An unsupervised sentence embedding method by mutual information maximization. In Empirical Methods in Natural Language Processing (EMNLP), pages 1601- 1610. <|md_end|>
--------------------------------------------------
<|box_start|>119 084 293 101<|box_end|><|ref_start|>title<|ref_end|><|md_start|># A Training Details<|md_end|>
<|box_start|>118 108 488 397<|box_end|><|ref_start|>text<|ref_end|><|md_start|>A Training DetailsWe implement SimCSE with transformers package (Wolf et al., 2020). For supervised SimCSE, we train our models for 3 epochs, evaluate the model every 250 training steps on the development set of STS- B and keep the best checkpoint for the final evaluation on test sets. We do the same for the unsupervised SimCSE, except that we train the model for one epoch. We carry out grid- search of batch size \(\in \{64,128,256,512\}\) and learning rate \(\in \{1e - 5,3e - 5,5e - 5\}\) on STS- B development set and adopt the hyperparameter settings in Table A.1. We find that SimCSE is not sensitive to batch sizes as long as tuning the learning rates accordingly, which contradicts the finding that contrastive learning requires large batch sizes (Chen et al., 2020). It is probably due to that all SimCSE models start from pre- trained checkpoints, which already provide us a good set of initial parameters.<|md_end|>
<|box_start|>119 502 484 516<|box_end|><|ref_start|>table_caption<|ref_end|><|md_start|>Table A.1: Batch sizes and learning rates for SimCSE.<|md_end|>
<|box_start|>120 408 480 487<|box_end|><|ref_start|>table<|ref_end|><|md_start|><ecel><fcel>Unsupervised<lcel><lcel><lcel><fcel>Supervised<lcel><lcel><nl>
<ucel><fcel>BERT base<fcel>large<fcel>RbBERTa base<fcel>large<fcel>base<fcel>large<ecel><nl>
<fcel>Batch size<fcel>64<fcel>64<fcel>512<fcel>512<fcel>512<fcel>512<ecel><nl>
<fcel>Learning rate<fcel>3e-5<fcel>1e-5<fcel>1e-5<fcel>3e-5<fcel>5e-5<fcel>1e-5<ecel><nl>
<|md_end|>
<|box_start|>118 531 487 643<|box_end|><|ref_start|>text<|ref_end|><|md_start|>For both unsupervised and supervised SimCSE, we take the [CLS] representation with an MLP layer on top of it as the sentence representation. Specially, for unsupervised SimCSE, we discard the MLP layer and only use the [CLS] output during test, since we find that it leads to better performance (ablation study in \(\S 6.3)\)<|md_end|>
<|box_start|>118 644 487 787<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Finally, we introduce one more optional variant which adds a masked language modeling (MLM) objective (Devlin et al., 2019) as an auxiliary loss to Eq.1: \(\ell +\lambda \cdot \ell^{\mathrm{mlm}}\) \(\lambda\) is a hyperparameter). This helps SimCSE avoid catastrophic forgetting of token- level knowledge. As we will show in Table D.2, we find that adding this term can help improve performance on transfer tasks (not on sentence- level STS tasks).<|md_end|>
<|box_start|>118 798 474 816<|box_end|><|ref_start|>title<|ref_end|><|md_start|># B Different Settings for STS Evaluation<|md_end|>
<|box_start|>116 823 488 887<|box_end|><|ref_start|>text<|ref_end|><|md_start|>We elaborate the differences in STS evaluation settings in previous work in terms of (a) whether to use additional regressors; (b) reported metrics; (c) different ways to aggregate results.<|md_end|>
<|box_start|>118 888 488 919<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Additional regressors. The default SentEval implementation applies a linear regressor on top of <|txt_contd|><|md_end|>
<|box_start|>513 248 883 291<|box_end|><|ref_start|>table_caption<|ref_end|><|md_start|>Table B.1: STS evaluation protocols used in different papers. "Reg.": whether an additional regressor is used; "aggr.": methods to aggregate different subset results.<|md_end|>
<|box_start|>517 083 875 232<|box_end|><|ref_start|>table<|ref_end|><|md_start|><fcel>Paper<fcel>Reg.<fcel>Metric<fcel>Aggr.<nl>
<fcel>Hill et al. (2016)<ecel><fcel>Both<fcel>all<nl>
<fcel>Conneau et al. (2017)<fcel>✓<fcel>Pearson<fcel>mean<nl>
<fcel>Conneau and Kiela (2018)<fcel>✓<fcel>Pearson<fcel>mean<nl>
<fcel>Reimers and Gurevych (2019)<ecel><fcel>Spearman<fcel>all<nl>
<fcel>Zhang et al. (2020)<ecel><fcel>Spearman<fcel>all<nl>
<fcel>Li et al. (2020)<ecel><fcel>Spearman<fcel>wmean<nl>
<fcel>Su et al. (2021)<ecel><fcel>Spearman<fcel>wmean<nl>
<fcel>Wieting et al. (2020)<ecel><fcel>Pearson<fcel>mean<nl>
<fcel>Giorgi et al. (2021)<ecel><fcel>Spearman<fcel>mean<nl>
<fcel>Ours<ecel><fcel>Spearman<fcel>all<nl>
<|md_end|>
<|box_start|>513 319 883 431<|box_end|><|ref_start|>text<|ref_end|><|md_start|>frozen sentence embeddings for STS- B and SICKR, and train the regressor on the training sets of the two tasks, while most sentence representation papers take the raw embeddings and evaluate in an unsupervised way. In our experiments, we do not apply any additional regressors and directly take cosine similarities for all STS tasks.<|md_end|>
<|box_start|>513 433 884 545<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Metrics. Both Pearson's and Spearman's correlation coefficients are used in the literature. Reimers et al. (2016) argue that Spearman correlation, which measures the rankings instead of the actual scores, better suits the need of evaluating sentence embeddings. For all of our experiments, we report Spearman's rank correlation.<|md_end|>
<|box_start|>513 548 883 868<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Aggregation methods. Given that each year's STS challenge contains several subsets, there are different choices to gather results from them: one way is to concatenate all the topics and report the overall Spearman's correlation (denoted as "all"), and the other is to calculate results for different subsets separately and average them (denoted as "mean" if it is simple average or "wmean" if weighted by the subset sizes). However, most papers do not claim the method they take, making it challenging for a fair comparison. We take some of the most recent work: SBERT (Reimers and Gurevych, 2019), BERT- flow (Li et al., 2020) and BERT- whitening (Su et al., 2021)12 as an example: In Table B.2, we compare our reproduced results to reported results of SBERT and BERT- whitening, and find that Reimers and Gurevych (2019) take the "all" setting but Li et al. (2020); Su et al. (2021) take the "wmean" setting, even though Li et al. (2020) claim that they take the same setting as Reimers<|md_end|>
--------------------------------------------------
<|box_start|>119 082 862 229<|box_end|><|ref_start|>table<|ref_end|><|md_start|><fcel>Model<fcel>STS12<fcel>STS13<fcel>STS14<fcel>STS15<fcel>STS16<fcel>STS-B<fcel>SICK-R<fcel>Avg.<nl>
<fcel>SBERT (all)<fcel>70.97<fcel>76.53<fcel>73.19<fcel>79.09<fcel>74.30<fcel>76.98<fcel>72.91<fcel>74.85<nl>
<fcel>SBERT (wmean)<fcel>66.35<fcel>73.76<fcel>73.88<fcel>77.33<fcel>73.62<fcel>76.98<fcel>72.91<fcel>73.55<nl>
<fcel>SBERT▲<fcel>70.97<fcel>76.53<fcel>73.19<fcel>79.09<fcel>74.30<fcel>77.03<fcel>72.91<fcel>74.89<nl>
<fcel>BERT-whitening (NLI, all)<fcel>57.83<fcel>66.90<fcel>60.89<fcel>75.08<fcel>71.30<fcel>68.23<fcel>63.73<fcel>66.28<nl>
<fcel>BERT-whitening (NLI, wmean)<fcel>61.43<fcel>65.90<fcel>65.96<fcel>74.80<fcel>73.10<fcel>68.23<fcel>63.73<fcel>67.59<nl>
<fcel>BERT-whitening (NLI)▲<fcel>61.69<fcel>65.70<fcel>66.02<fcel>75.11<fcel>73.11<fcel>68.19<fcel>63.60<fcel>67.63<nl>
<fcel>BERT-whitening (target, all)<fcel>42.88<fcel>77.77<fcel>66.27<fcel>63.60<fcel>67.58<fcel>71.34<fcel>60.40<fcel>64.26<nl>
<fcel>BERT-whitening (target, wmean)<fcel>63.38<fcel>73.01<fcel>69.13<fcel>74.48<fcel>72.56<fcel>71.34<fcel>60.40<fcel>69.19<nl>
<fcel>BERT-whitening (target)▲<fcel>63.62<fcel>73.02<fcel>69.23<fcel>74.52<fcel>72.15<fcel>71.34<fcel>60.60<fcel>69.21<nl>
<|md_end|>
<|box_start|>118 245 884 287<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Table B.2: Comparisons of our reproduced results using different evaluation protocols and the original numbers. \(\spadesuit\) : results from Reimers and Gurevych (2019); \(\spadesuit\) : results from Su et al. (2021); Other results are reproduced by us. From the table we see that SBERT takes the "all' evaluation and BERT- whitening takes the "wmean' evaluation.<|md_end|>
<|box_start|>151 506 835 520<|box_end|><|ref_start|>table_caption<|ref_end|><|md_start|>Table B.3: STS results with "wmean" setting (Spearman). \(\spadesuit\) : from Li et al. (2020); Su et al. (2021).<|md_end|>
<|box_start|>148 300 852 491<|box_end|><|ref_start|>table<|ref_end|><|md_start|><fcel>Model<fcel>STS12<fcel>STS13<fcel>STS14<fcel>STS15<fcel>STS16<fcel>STS-B<fcel>SICK-R<fcel>Avg.<nl>
<fcel>BERTbase (first-last avg.)▲<fcel>57.86<fcel>61.97<fcel>62.49<fcel>70.96<fcel>69.76<fcel>59.04<fcel>63.75<fcel>63.69<nl>
<fcel>+ flow (NLI)▲<fcel>59.54<fcel>64.69<fcel>64.66<fcel>72.92<fcel>71.84<fcel>58.56<fcel>65.44<fcel>65.38<nl>
<fcel>+ flow (target)▲<fcel>63.48<fcel>72.14<fcel>68.42<fcel>73.77<fcel>75.37<fcel>70.72<fcel>63.11<fcel>69.57<nl>
<fcel>+ whitening (NLI)▲<fcel>61.69<fcel>65.70<fcel>66.02<fcel>75.11<fcel>73.11<fcel>68.19<fcel>63.60<fcel>67.63<nl>
<fcel>+ whitening (target)▲<fcel>63.62<fcel>73.02<fcel>69.23<fcel>74.52<fcel>72.15<fcel>71.34<fcel>60.60<fcel>69.21<nl>
<fcel>* Unsup. SimCSE-BERTbase<fcel>70.14<fcel>79.56<fcel>75.91<fcel>81.46<fcel>79.07<fcel>76.85<fcel>72.23<fcel>76.46<nl>
<fcel>SBERTbase (first-last avg.)▲<fcel>68.70<fcel>74.37<fcel>74.73<fcel>79.65<fcel>75.21<fcel>77.63<fcel>74.84<fcel>75.02<nl>
<fcel>+ flow (NLI)▲<fcel>67.75<fcel>76.73<fcel>75.53<fcel>80.63<fcel>77.58<fcel>79.10<fcel>78.03<fcel>76.48<nl>
<fcel>+ flow (target)▲<fcel>68.95<fcel>78.48<fcel>77.62<fcel>81.95<fcel>78.94<fcel>81.13<fcel>74.97<fcel>77.42<nl>
<fcel>+ whitening (NLI)▲<fcel>69.11<fcel>75.79<fcel>75.76<fcel>82.31<fcel>79.61<fcel>78.66<fcel>76.33<fcel>76.80<nl>
<fcel>+ whitening (target)▲<fcel>69.01<fcel>78.10<fcel>77.04<fcel>80.83<fcel>77.93<fcel>80.50<fcel>72.54<fcel>76.56<nl>
<fcel>* Sup. SimCSE-BERTbase<fcel>70.90<fcel>81.49<fcel>80.19<fcel>83.79<fcel>81.89<fcel>84.25<fcel>80.39<fcel>80.41<nl>
<|md_end|>
<|box_start|>118 547 488 610<|box_end|><|ref_start|>text<|ref_end|><|md_start|>and Gurevych (2019). Since the "all' setting fuses data from different topics together, it makes the evaluation closer to real- world scenarios, and unless specified, we take the "all' setting.<|md_end|>
<|box_start|>118 611 489 851<|box_end|><|ref_start|>text<|ref_end|><|md_start|>We list evaluation settings for a number of previous work in Table B.1. Some of the settings are reported by the paper and some of them are inferred by comparing the results and checking their code. As we can see, the evaluation protocols are very incoherent across different papers. We call for unifying the setting in evaluating sentence embeddings for future research. We will also release our evaluation code for better reproducibility. Since previous work uses different evaluation protocols from ours, we further evaluate our models in these settings to make a direct comparison to the published numbers. We evaluate SimCSE with "wmean" and Spearman's correlation to directly compare to Li et al. (2020) and Su et al. (2021) in Table B.3. <|md_end|>
<|box_start|>119 862 292 879<|box_end|><|ref_start|>title<|ref_end|><|md_start|># C Baseline Models<|md_end|>
<|box_start|>118 888 487 918<|box_end|><|ref_start|>text<|ref_end|><|md_start|>We elaborate on how we obtain different baselines for comparison in our experiments:<|md_end|>
<|box_start|>533 547 884 641<|box_end|><|ref_start|>text<|ref_end|><|md_start|>For average GroVe embedding (Pennington et al., 2014), InferSent (Conneau et al., 2017) and Universal Sentence Encoder (Cer et al., 2018), we directly report the results from Reimers and Gurevych (2019), since our evaluation setting is the same as theirs.<|md_end|>
<|box_start|>533 660 883 739<|box_end|><|ref_start|>text<|ref_end|><|md_start|>For BERT (Dewlin et al., 2019) and RoBERTa (Liu et al., 2019), we download the pretrained model weights from HuggingFace's Transformers13, and evaluate the models with our own scripts.<|md_end|>
<|box_start|>533 757 884 868<|box_end|><|ref_start|>text<|ref_end|><|md_start|>For SBERT and SRoBERTa (Reimers and Gurevych, 2019), we reuse the results from the original paper. For results not reported by Reimers and Gurevych (2019), such as the performance of SRoBERTa on transfer tasks, we download the model weights from SentenceTransformers14 and evaluate them.<|md_end|>
--------------------------------------------------
<|box_start|>128 245 871 260<|box_end|><|ref_start|>table_caption<|ref_end|><|md_start|>Table C.1: Comparison of using NLI or target data for postprocessing methods "all", Spearman's correlation).<|md_end|>
<|box_start|>152 082 842 230<|box_end|><|ref_start|>table<|ref_end|><|md_start|><fcel>Model<fcel>STS12<fcel>STS13<fcel>STS14<fcel>STS15<fcel>STS16<fcel>STS-B<fcel>SICK-R<fcel>Avg.<nl>
<fcel>BERT-flow (NLI)<fcel>58.40<fcel>67.10<fcel>60.85<fcel>75.16<fcel>71.22<fcel>68.66<fcel>64.47<fcel>66.55<nl>
<fcel>BERT-flow (target)<fcel>53.15<fcel>78.38<fcel>66.02<fcel>62.09<fcel>70.84<fcel>71.70<fcel>61.97<fcel>66.31<nl>
<fcel>BERT-whitening (NLI)<fcel>57.83<fcel>66.90<fcel>60.90<fcel>75.08<fcel>71.31<fcel>68.24<fcel>63.73<fcel>66.28<nl>
<fcel>BERT-whitening (target)<fcel>42.88<fcel>77.77<fcel>66.28<fcel>63.60<fcel>67.58<fcel>71.34<fcel>60.40<fcel>64.26<nl>
<fcel>SBERT-flow (NLI)<fcel>69.78<fcel>77.27<fcel>74.35<fcel>82.01<fcel>77.46<fcel>79.12<fcel>76.21<fcel>76.60<nl>
<fcel>SBERT-flow (target)<fcel>66.18<fcel>82.69<fcel>76.22<fcel>73.72<fcel>75.71<fcel>79.99<fcel>73.82<fcel>75.48<nl>
<fcel>SBERT-whitening (NLI)<fcel>69.65<fcel>77.57<fcel>74.66<fcel>82.27<fcel>78.39<fcel>79.52<fcel>76.91<fcel>77.00<nl>
<fcel>SBERT-whitening (target)<fcel>52.91<fcel>81.91<fcel>75.44<fcel>72.24<fcel>72.93<fcel>80.50<fcel>72.54<fcel>72.64<nl>
<|md_end|>
<|box_start|>128 283 466 325<|box_end|><|ref_start|>table<|ref_end|><|md_start|><fcel>τ<fcel>N/A<fcel>0.001<fcel>0.01<fcel>0.05<fcel>0.1<fcel>1<nl>
<fcel>STS-B<fcel>85.9<fcel>84.9<fcel>85.4<fcel>86.2<fcel>82.0<fcel>64.0<nl>
<|md_end|>
<|box_start|>116 340 489 383<|box_end|><|ref_start|>table_footnote<|ref_end|><|md_start|>Table D.1: STS-B development results (Spearman's correlation) with different temperatures."N/A":Dot product instead of cosine similarity.<|md_end|>
<|box_start|>138 407 489 637<|box_end|><|ref_start|>text<|ref_end|><|md_start|>For DeCLUTR (Giorgi et al., 2021) and contrastive tension (Carlsson et al., 2021), we reevaluate their checkpoints in our setting. For BERT- flow (Li et al., 2020), since their original numbers take a different setting, we retrain their models using their code<sup>15</sup>, and evaluate the models using our own script. For BERT- whitening (Su et al., 2021), we implemented our own version of whitening script following the same pooling method in Su et al. (2021), i.e. first- last average pooling. Our implementation can reproduce the results from the original paper (see Table B.2).<|md_end|>
<|box_start|>116 645 488 758<|box_end|><|ref_start|>text<|ref_end|><|md_start|>For both BERT- flow and BERT- whitening, they have two variants of postprocessing: one takes the NLI data ("NLI") and one directly learns the embedding distribution on the target sets ("target"). We find that in our evaluation setting, "target" is generally worse than "NLI" (Table C.1), so we only report the NLI variant in the main results.<|md_end|>
<|box_start|>118 769 297 785<|box_end|><|ref_start|>title<|ref_end|><|md_start|># D Ablation Studies<|md_end|>
<|box_start|>116 801 488 897<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Normalization and temperature. We train SimCSE using both dot product and cosine similarity with different temperatures and evaluate them on the STS- B development set. As shown in Table D.1, with a carefully tuned temperature \(\tau = 0.05\), cosine similarity is better than dot product.<|md_end|>
<|box_start|>513 399 882 429<|box_end|><|ref_start|>table_caption<|ref_end|><|md_start|>Table D.2: Ablation studies of the MLM objective based on the development sets using BERTbase.<|md_end|>
<|box_start|>549 283 842 384<|box_end|><|ref_start|>table<|ref_end|><|md_start|><fcel>Model<fcel>STS-B<fcel>Avg. transfer<nl>
<fcel>w/o MLM<fcel>86.2<fcel>85.8<nl>
<fcel>w/MLM<ecel><ecel><nl>
<fcel>λ = 0.01<fcel>85.7<fcel>86.1<nl>
<fcel>λ = 0.1<fcel>85.7<fcel>86.2<nl>
<fcel>λ = 1<fcel>85.1<fcel>85.8<nl>
<|md_end|>
<|box_start|>513 455 883 552<|box_end|><|ref_start|>text<|ref_end|><|md_start|>MLM auxiliary task. Finally, we study the impact of the MLM auxiliary objective with different \(\lambda\). As shown in Table D.2, the token- level MLM objective improves the averaged performance on transfer tasks modestly, yet it brings a consistent drop in semantic textual similarity tasks.<|md_end|>
<|box_start|>514 565 677 581<|box_end|><|ref_start|>title<|ref_end|><|md_start|># E Transfer Tasks<|md_end|>
<|box_start|>513 592 884 736<|box_end|><|ref_start|>text<|ref_end|><|md_start|>We evaluate our models on the following transfer tasks: MR (Pang and Lee, 2005), CR (Hu and Liu, 2004), SUBJ (Pang and Lee, 2004), MPQA (Wiebe et al., 2005), SST- 2 (Socher et al., 2013), TREC (Voorhees and Tice, 2000) and MRPC (Dolan and Brockett, 2005). A logistic regression classifier is trained on top of (frozen) sentence embeddings produced by different methods. We follow default configurations from SentEval<sup>16</sup>.<|md_end|>
<|box_start|>513 738 884 882<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Table E.1 shows the evaluation results on transfer tasks. We find that supervised SimCSE performs on par or better than previous approaches, although the trend of unsupervised models remains unclear. We find that adding this MLM term consistently improves performance on transfer tasks, confirming our intuition that sentence- level objective may not directly benefit transfer tasks. We also experiment with post- processing methods (BERT- <|md_end|>
--------------------------------------------------
<|box_start|>119 462 884 505<|box_end|><|ref_start|>table_caption<|ref_end|><|md_start|>Table E.1: Transfer task results of different sentence embedding models (measured as accuracy). \(\clubsuit\) : results from Reimers and Gurevych (2019). \(\heartsuit\) : results from Zhang et al. (2020). We highlight the highest numbers among models with the same pre-trained encoder: MLM adding MLM as an auxiliary task with \(\lambda = 0.1\)<|md_end|>
<|box_start|>162 082 836 447<|box_end|><|ref_start|>table<|ref_end|><|md_start|><fcel>Model<fcel>MR<fcel>CR<fcel>SUBJ<fcel>MPQA<fcel>SST<fcel>TREC<fcel>MRPC<fcel>Avg.<nl>
<fcel>Unsupervised models<lcel><lcel><lcel><lcel><lcel><lcel><lcel><lcel><nl>
<fcel>GloVe embeddings (avg.)▲<fcel>77.25<fcel>78.30<fcel>91.17<fcel>87.85<fcel>80.18<fcel>83.00<fcel>72.87<fcel>81.52<nl>
<fcel>Skip-thought○<fcel>76.50<fcel>80.10<fcel>93.60<fcel>87.10<fcel>82.00<fcel>92.20<fcel>73.00<fcel>83.50<nl>
<fcel>Avg. BERT embeddings▲<fcel>78.66<fcel>86.25<fcel>94.37<fcel>88.66<fcel>84.40<fcel>92.80<fcel>69.54<fcel>84.94<nl>
<fcel>BERT- [CLS] embeddings▲<fcel>78.68<fcel>84.85<fcel>94.21<fcel>88.23<fcel>84.13<fcel>91.40<fcel>71.13<fcel>84.66<nl>
<fcel>IS-BERTbase<fcel>81.09<fcel>87.18<fcel>94.96<fcel>88.75<fcel>85.96<fcel>88.64<fcel>74.24<fcel>85.83<nl>
<fcel>* SimCSE-BERTbase<fcel>81.18<fcel>86.46<fcel>94.45<fcel>88.88<fcel>85.50<fcel>89.80<fcel>74.43<fcel>85.81<nl>
<fcel>w/ MLM<fcel>82.92<fcel>87.23<fcel>95.71<fcel>88.73<fcel>86.81<fcel>87.01<fcel>78.07<fcel>86.64<nl>
<fcel>* SimCSE-RoBERTbase<fcel>81.04<fcel>87.74<fcel>93.28<fcel>86.94<fcel>86.60<fcel>84.60<fcel>73.68<fcel>84.84<nl>
<fcel>w/ MLM<fcel>83.27<fcel>87.76<fcel>95.05<fcel>87.16<fcel>89.02<fcel>90.80<fcel>75.13<fcel>86.90<nl>
<fcel>* SimCSE-RoBERTlarge<fcel>82.74<fcel>87.87<fcel>93.66<fcel>88.22<fcel>88.58<fcel>92.00<fcel>69.68<fcel>86.11<nl>
<fcel>w/ MLM<fcel>84.66<fcel>88.56<fcel>95.43<fcel>87.50<fcel>89.46<fcel>95.00<fcel>72.41<fcel>87.57<nl>
<fcel>Supervised models<lcel><lcel><lcel><lcel><lcel><lcel><lcel><lcel><nl>
<fcel>InferSent-GloVe▲<fcel>81.57<fcel>86.54<fcel>92.50<fcel>90.38<fcel>84.18<fcel>88.20<fcel>75.77<fcel>85.59<nl>
<fcel>Universal Sentence Encoder▲<fcel>80.09<fcel>85.19<fcel>93.98<fcel>86.70<fcel>86.38<fcel>93.20<fcel>70.14<fcel>85.10<nl>
<fcel>SBERTbase▲<fcel>83.64<fcel>89.43<fcel>94.39<fcel>89.86<fcel>88.96<fcel>89.60<fcel>76.00<fcel>87.41<nl>
<fcel>* SimCSE-BERTbase<fcel>82.69<fcel>89.25<fcel>94.81<fcel>89.59<fcel>87.31<fcel>88.40<fcel>73.51<fcel>86.51<nl>
<fcel>w/ MLM<fcel>82.68<fcel>88.88<fcel>94.52<fcel>89.82<fcel>88.41<fcel>87.60<fcel>76.12<fcel>86.86<nl>
<fcel>SRoBERTbase<fcel>84.91<fcel>90.83<fcel>92.56<fcel>88.75<fcel>90.50<fcel>88.60<fcel>78.14<fcel>87.76<nl>
<fcel>* SimCSE-RoBERTbase<fcel>84.92<fcel>92.00<fcel>94.11<fcel>89.82<fcel>91.27<fcel>88.80<fcel>75.65<fcel>88.08<nl>
<fcel>w/ MLM<fcel>85.08<fcel>91.76<fcel>94.02<fcel>89.72<fcel>92.31<fcel>91.20<fcel>76.52<fcel>88.66<nl>
<fcel>* SimCSE-RoBERTlarge<fcel>88.12<fcel>92.37<fcel>95.11<fcel>90.49<fcel>92.75<fcel>91.80<fcel>76.64<fcel>89.61<nl>
<fcel>w/ MLM<fcel>88.45<fcel>92.53<fcel>95.19<fcel>90.58<fcel>93.30<fcel>93.80<fcel>77.74<fcel>90.23<nl>
<|md_end|>
<|box_start|>118 531 488 643<|box_end|><|ref_start|>text<|ref_end|><|md_start|>flow/whitening) and find that they both hurt performance compared to their base models, showing that good uniformity of representations does not lead to better embeddings for transfer learning. As we argued earlier, we think that transfer tasks are not a major goal for sentence embeddings, and thus we take the STS results for main comparison.<|md_end|>
<|box_start|>119 657 419 674<|box_end|><|ref_start|>title<|ref_end|><|md_start|># F Distribution of Singular Values<|md_end|>
<|box_start|>118 685 487 830<|box_end|><|ref_start|>text<|ref_end|><|md_start|>Figure F.1 shows the singular value distribution of SimCSE together with other baselines. For both unsupervised and supervised cases, singular value drops the fastest for vanilla BERT or SBERT embeddings, while SimCSE helps flatten the spectrum distribution. Postprocessing- based methods such as BERT- flow or BERT- whitening flatten the curve even more since they directly aim for the goal of mapping embeddings to an isotropic distribution.<|md_end|>
<|box_start|>119 844 412 860<|box_end|><|ref_start|>title<|ref_end|><|md_start|># G Cosine-similarity Distribution<|md_end|>
<|box_start|>118 872 488 919<|box_end|><|ref_start|>text<|ref_end|><|md_start|>To directly show the strengths of our approaches on STS tasks, we illustrate the cosine similarity distributions of STS- B pairs with different groups of <|txt_contd|><|md_end|>
<|box_start|>521 532 874 670<|box_end|><|ref_start|>image<|ref_end|><|md_start|>![]('img_url')<|md_end|>
<|box_start|>512 680 884 721<|box_end|><|ref_start|>image_caption<|ref_end|><|md_start|>Figure F.1: Singular value distributions of sentence embedding matrix from sentences in STS-B. We normalize the singular values so that the largest one is 1.<|md_end|>
<|box_start|>513 743 884 918<|box_end|><|ref_start|>text<|ref_end|><|md_start|>human ratings in Figure G.1. Compared to all the baseline models, both unsupervised and supervised SimCSE better distinguish sentence pairs with different levels of similarities, thus leading to a better performance on STS tasks. In addition, we observe that SimCSE generally shows a more scattered distribution than BERT or SBERT, but also preserves a lower variance on semantically similar sentence pairs compared to whitened distribution. This observation further validates that SimCSE can achieve a better alignment- uniformity balance.<|md_end|>
--------------------------------------------------
<|box_start|>147 346 860 616<|box_end|><|ref_start|>image<|ref_end|><|md_start|>![]('img_url')<|md_end|>
<|box_start|>115 625 883 655<|box_end|><|ref_start|>image_caption<|ref_end|><|md_start|>Figure G.1: Density plots of cosine similarities between sentence pairs in STS-B. Pairs are divided into 5 groups based on ground truth ratings (higher means more similar) along the y-axis, and x-axis is the cosine similarity.<|md_end|>